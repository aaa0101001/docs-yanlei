## Kafka

消费者：关闭自动提交offset

- 1、给topic设置relication.factor参数：该值必须大于1，要求每个partition必须有至少两个副本
- 2、在kafka服务端时设置min.insync.replicas参数：该值必须大于1，这个是要求一个leader至少感知到有一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower
- 3、在producer端设置 acks=all ：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了
- 4、在producer端设置retries=MAX（很大一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限次重试，卡在这里了
- 生产者：若设置 acks=all ，一定不会丢

## 消息顺序问题

- 1、一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不	会用这个
- 2、写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性

如何解决消息队列的延时以及过期失效问题？
1、死信队列
2、延时插件

## 消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？

造成原因：消费端出现问题，导致消费速度特别慢
解决：临时紧急扩容
1、先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉
新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量
然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue
接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据
等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息

## mq 中的消息过期失效了

造成原因：高峰期如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉
解决：过了高峰期之后，比如凌晨，手动找出丢失的数据并重新放入mq中消费

假设你用的是 RabbitMQ，RabbitMQ 是可以设置过期时间的，也就是 TTL如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了那这就是第二个坑了这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢

这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来也只能是这样了

假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次

mq 都快写满了

## 消息积压

- 如果消息积压在 mq里，你很长时间都没有处理掉，此时导致mq都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息然后走第二个方案，到了晚上再补数据吧

## 消息模型

- 基本消息模型：生产者–>队列–>消费者
- work消息模型：生产者–>队列–>多个消费者共同消费
- 订阅模型-Fanout：广播模式，将消息交给所有绑定到交换机的队列，每个消费者都会收到同一条消息
- 订阅模型-Direct：定向，把消息交给符合指定 rotingKey 的队列
- 订阅模型-Topic 主题模式：通配符，把消息交给符合routing pattern（路由模式） 的队列
- 第6种其实是RPC，并不是MQ

